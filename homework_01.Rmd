---
title: "PSTAT 231 - Homework 1"
author: "Tomas"
output:
  html_document:
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Machine Learning Main Ideas
### Question 1:
  **Define supervised and unsupervised learning. What are the difference(s) between them?**
  
  For supervised learning, each observation of the predictor measurement(s) $x_i$ is associated with a response measurement $y_i$. The objective is to fit a model that relates the response to the predictors, with the goal of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference). Linear and logistic regression are examples of supervised learning methods.

  On the other hand, unsupervised learning describes the situation in which there is no associated response $y_i$ for any observation of the predictor measurement(s) $x_i$. In this case, there is no response variable that can be used to supervise a statistical analysis. However, it is possible to attempt to identify the relationships between observations. One approach is cluster analysis, in which the goal is to determine if observations fall into relatively distinct groups.
  
  Source: Pages 26-27 of ISLR

### Question 2:
  **Explain the difference between a regression model and a classification model, specifically in the context of machine learning.**
  
  Variables can be characterized as either quantitative or qualitative; quantitative variables take on numerical values (age, height, etc.) while qualitative variables take on values in one of $K$ different categories (marital status, grade, etc.). Problems with a quantitative response tend to be classified as regressions problems while problems with a qualitative response tend to be classified as classification problems. However, the distinction is not that crisp. Least squares linear regression is used with a quantitative response while logistic regression is typically used with a qualitative response. Thus, logistic regression is a classification method; it can be considered a regression method as well, as it estimates class probabilities. Some statistical methods, such as K-nearest neighbors and boosting, can be used in the case of either quantitative or qualitative responses.

  Source: Pages 28-29 of ISLR

### Question 3:
  **Name two commonly used metrics for regression ML problems. Name two commonly used metrics for classification ML problems.**
  
  Two commonly used metrics for regression ML problems are the Training and Test Mean Squared Error (MSE). Two commonly used metrics for classification ML problems are the Training and Test Error Rate.

  Source: Slide 53 of Lecture 2

### Question 4:
  **As discussed, statistical models can be used for different purposes. These purposes can generally be classified into the following three categories. Provide a brief description of each.**

  * Descriptive models: Descriptive models aim to visually emphasize a trend in data. For example, a linear model may be used to identify a trend on a scatterplot.
  * Inferential models: Inferential models aim to test theories; in particular, its objective is to answer "What features are significant?". They aim to state the relationship between outcome and predictor(s) and possibly identify causilty.
  * Predictive models: Predictive models aim to predict the response with minimum reducible error; in particular, its objective is to answer "What combination of features fits best?". It is not focused on hypothesis tests.

  Source: Slide 7 of Lecture 2

### Question 5:
  **Predictive models are frequently used in machine learning, and they can usually be described as either mechanistic or empirically-driven. Answer the following questions.**

  * **Define mechanistic. Define empirically-driven. How do these model types differ? How are they similar?** Mechanistic (parametric) models and empirically-driven (non-parametric) models leverage training data in order to estimate an unknown function $f(X)$. Parametric models convert the problem of estimating $f$ into one of estimating a set of parameters. To develop a parametric model, one makes an assumption about the functional form of $f$; for example, one approach is to assume $f$ is linear ($f(X) = \beta_0 + \beta_1 X_1 + \dots + \beta_p X_p$). Then, some pre-defined procedure is utiized the training data to fit (i.e. estimate the parameters that best fit the training data); one approach that is commonly in the aforementioned linear case is (ordinary) least squares (though it is not the only approach).
  Non-parametric methods do not make explicit assumptions about the functional form of $f$. This type of method attempts to identify an estimate of $f$ that gets as close to the data points as possible without being too rough or wiggly.
  Recall that parametric models require that an assumption be made about the functional form of $f$. One benefit of this is that it is generally easier to estimate a set of parameters than it is to fit an entirely arbitrary function $f$. However, the flexibility of a parametric model is dependent on the assumption that is made about the functional form of $f$. Thus, it is not unlikely for a parametric model to not coincide with $f$. If a parametric model is too far from $f$, then any estimates computed via the model will be poor. One way to remedy this issue is by increasing the number of parameters. However, adding too many parameters can lead to overfitting.
  Since non-parametric models are not dependent on any assumption about the functional form of $f$, this type of models have the potential to accurately fit a wider range of possible shapes for $f$. However, this comes with the cost of requiring a large number of observations to obtain an accurate estimate for $f$. Additionally, non-parametric models are prone to overfitting (like parametric models).
  * **In general, is a mechanistic or empirically-driven model easier to understand? Explain your choice.** Mechanistic models are generally easier to understand. This is due to the fact that it is relatively easy to understand the relationship between a response $Y$ and and its corresponding predictor measurement(s) $X = (X_1, \dots, X_p)$. When working with highly flexible models (such as empirically-driven models), it can be difficult to understand how any individual predictor is associated with the response; this is due to the fact that non-parametric models can lead to complicated estimates of $f$.
  * **Describe how the bias-variance tradeoff is related to the use of mechanistic or empirically-driven models.** Recall that mechanistic models are generally more simple (i.e. less flexible) than empirically-driven models. This implies that mechanistic models can be expected to achieve high bias and low variance. On the other hand, empirically-driven models can be expected to achieve low bias and high variance.

Source: Pages 21-26 of ISLR, Slide 37 of Lecture 2

### Question 6:
  **A political candidate’s campaign has collected some detailed voter history data from their constituents. The campaign is interested in two questions:**

  * **Given a voter’s profile/data, how likely is it that they will vote in favor of the candidate?**
  * **How would a voter’s likelihood of support for the candidate change if they had personal contact with the candidate?**

**Classify each question as either predictive or inferential. Explain your reasoning for each.**

The first question is predictive. The problem consists in leveraging a predictors (voter profile/data) to predict a response (likelihood voter will vote in favor of the candidate). This question does not aim to explain why a voter would vote for or against the candidate. The second question is inferential. The problem consists in identifying the relationship (namely causality) between predictors (a voter's contact with the candidate) and response (a voter's support of the candidate).

Source: Slide 7 of Lecture 2

## Exploratory Data Analysis
```{r, include=FALSE}
#install.packages("tidyverse")
#install.packages("tidymodels")
#install.packages("ISLR")

library(tidyverse)
library(tidymodels)
library(ISLR)
```

### Exercise 1:
  **We are interested in highway miles per gallon, or the hwy variable. Create a histogram of this variable. Describe what you see/learn.** It can be observed that most cars can travel 15-20 or 25-30 miles per gallon on the highway. Few cars in the mpg data set can travel 35-45 miles per gallon on the highway. The may mean that most cars may be commuter cars.
```{r, eval = TRUE, indent = "  "}
hist(mpg$hwy, main="Highway miles per gallon of cars in mpg dataset", xlab="Highway miles per gallon")
```

### Exercise 2:
  **Create a scatterplot. Put hwy on the x-axis and cty on the y-axis. Describe what you notice. Is there a relationship between hwy and cty? What does this mean?** There appears to be a positive linear correlation between hwy and cty. Based on the observation in the mpg data set, one would expect cars that achieve high hwy to achieve high cty, and cars that achieve low hwy to achieve low cty. This makes sense as one would expect engine performance to remain consistent across different terrains.
```{r, eval = TRUE, indent = "  "}
plot(mpg$hwy, mpg$cty, main="cty vs. hwy", xlab="hwy ", ylab="cty") 
```

### Exercise 3:
  **Make a bar plot of manufacturer. Flip it so that the manufacturers are on the y-axis. Order the bars by height. Which manufacturer produced the most cars? Which produced the least?** The bars were ordered in terms of decreasing height; it was unclear if they should ordered in increasing or decreasing order. Dodge produced the most cars while Lincoln produced the least.
```{r, eval = TRUE, indent = "  "}
count_manufacturer_frequency = function(value){
  return (length(which(mpg$manufacturer == value)))
}

names = unique(mpg$manufacturer)
freqs = sort(sapply(names, count_manufacturer_frequency))
barplot(freqs, main="Manufacturer Production", xlab="Frequency", ylab="Manufacturers", horiz=TRUE, names.arg=names(freqs), las=1, cex.names=0.5)
```

### Exercise 4:
  **Make a box plot of hwy, grouped by cyl. Do you see a pattern? If so, what?** It appears that the highway miles per gallon tends to decrease as the number of cylinders increases.
```{r, eval = TRUE, indent = "  "}
boxplot(mpg$hwy ~ mpg$cyl, main="hwy grouped by cyl", xlab="cyl", ylab="hwy") 
```

### Exercise 5:
  **Use the corrplot package to make a lower triangle correlation matrix of the mpg dataset. (Hint: You can find information on the package here.) Which variables are positively or negatively correlated with which others? Do these relationships make sense to you? Are there any that surprise you?** All non-numeric data was ignored when constructing the lower triangle correlation matrix; only the displ, year, cyl, cty, and hwy columns were considered.
  
  The following variable pairs are positively correlated with each other: (hwy, cty), (displ, cyl). One would expect hwy to increase as cty increases. Additionally, displ should increase as cyl increases as displ refers to the combined swept volume of the pistons inside the cylinders of an engine. 

  The following variable pairs are negatively correlated with each other: (cyl, cty), (displ, cty), (hwy, cyl), (hwy, displ). On the other hand, the negative correlation of (cyl, cty) and (hwy, cyl) makes sense has more gas is consumed as the number of cylinders increases; this results in a car being able to travel less distance as the number of cylinders increases. By the aforementioned relationship between displ and cyl, it also makes sense that the pairs (displ, cty) and (hwy, displ) are negatively correlated.
  
  The year variable is not correlated with any other variable. This is somewhat surprising as one would expect milage per gallon to increase over time. However, my knowledge of car engines is fairly limited.
```{r, include=FALSE}
# install.packages('corrplot')

library(corrplot)
```
```{r, eval = TRUE, indent = "  "}
M = mpg %>% 
  select_if(is.numeric) %>%
  cor(.)
corrplot(M, method="number", type="lower", order='alphabet', diag=TRUE)
```

### Exercise 6:
  **Recreate the following graphic, as closely as you can.**
```{r, eval = TRUE, indent = "  "}
ggplot(mpg, aes(x=class, y=hwy)) +
  geom_boxplot() +
  labs(x="Vehicle Class", y="Highway MPG") +
  coord_flip() +
  geom_point(alpha=0.1, stroke=0.5, size=2) +
  geom_jitter(position=position_jitter(0.4)) +
  theme(
    panel.background = element_rect(fill = NA),
    panel.grid.major = element_line(colour = "grey90"),
    axis.line.y.left = element_line(colour = "white"),
    axis.line.x.bottom = element_line(colour = "grey10"),
    axis.ticks = element_blank(),
    axis.text=element_text(size=14, colour = "grey50"),
    axis.title=element_text(size=14, colour = "grey50")
  )
```

### Exercise 7:
  **Recreate the following graphic.**
```{r, eval = TRUE, indent = "  "}
ggplot(mpg, aes(x=class, y=hwy, fill=drv)) + 
    geom_boxplot()
```

### Exercise 8:
  **Recreate the following graphic.**
```{r, eval = TRUE, indent = "  "}
ggplot(mpg, mapping=aes(x=displ, y=hwy)) +
  geom_point(mapping=aes(color=drv)) +
  geom_smooth(method="loess", formula=y~x, mapping=aes(x=displ, y=hwy, linetype=drv), se=FALSE)
```
